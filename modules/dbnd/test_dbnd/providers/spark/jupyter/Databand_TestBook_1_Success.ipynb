{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8265b24-c02b-4013-8df8-9f86337342c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Install and config databand\n",
    "\n",
    "Before using databand tracking, you have to install and enable it. There is some enviromental variables to set:\n",
    "- DBND__TRACKING : enable dbnd tracking\n",
    "- DBND__CORE__DATABAND_URL : adress to your databand aplication\n",
    "- DBND__CORE__DATABAND_ACCESS_TOKEN : access token to your aplication, possible to generate in the application\n",
    "- DBND__ENABLE__SPARK_CONTEXT_ENV : enable tracking Spark features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb568970-81e4-41ed-97b1-36d0b7a80f2e",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install dbnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92abe5e4-a26c-43d2-925b-1d52e9adf815",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['DBND__TRACKING'] = 'True'\n",
    "os.environ['DBND__CORE__DATABAND_URL'] = 'X'\n",
    "os.environ['DBND__CORE__DATABAND_ACCESS_TOKEN'] = 'X'\n",
    "os.environ['DBND__ENABLE__SPARK_CONTEXT_ENV'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d2ebde7-78e8-418e-91af-266f6431ce5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### (Optional) Configure tracking parameters\n",
    "\n",
    "Tracked tasks are easier to monitor in the app when you give them their own names, you can configure: \n",
    "- JOB : job name (pipeline)\n",
    "- PROJECT : project name\n",
    "- NAME : run name\n",
    "\n",
    "More about tracking parameters in documentation\n",
    "\n",
    "https://www.ibm.com/docs/en/dobd?topic=integrations-python#tracking-parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b25d8bd7-e807-4cf5-bce0-ac0af257afc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.environ['DBND__TRACKING__PROJECT'] = 'Databricks Spark Examples'\n",
    "os.environ['DBND__RUN_INFO__NAME'] = 'Example spark tracking'\n",
    "os.environ['DBND__TRACKING__JOB'] = 'Spark Examples'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37902b8e-f6cd-45a3-99bf-f9ea83ae2c9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Install and import Spark and Databand dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8324de00-db09-4b92-8874-a14b68cafff1",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from dbnd import dbnd_tracking, log_dataset_op, task, dbnd_tracking_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb9b3243-d605-4243-82df-6c99b3ed435d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Tracking dataset operations\n",
    "## Scenerio Read data -> Modify -> Write\n",
    "\n",
    "**Important! Please run all cells above, and run selected example. If you want try another example, please rerun all cells above. Otherwise, results may overwrite in databand app.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e73a8ac-8860-4eb0-b582-7709fd2e49c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Example 1 - Track snippet of code\n",
    "\n",
    "The simplest way how to track some dataset operations for any snippet \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c51002d2-fdde-45d4-82c0-b2c7ca4da9c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "os.environ['DBND__TRACKING__JOB'] = 'Spark Example 1'\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SimplePlaybook\").getOrCreate()\n",
    "\n",
    "with dbnd_tracking():\n",
    "    # Read operation\n",
    "    data = [\n",
    "            (\"2024-08-01\", 150),\n",
    "            (\"2024-08-02\", 50),\n",
    "            (\"2024-08-03\", 200),\n",
    "            (\"2024-08-04\", 75)\n",
    "        ]\n",
    "\n",
    "    columns = [\"transaction_date\", \"transaction_amount\"]\n",
    "    data = spark.createDataFrame(data, schema=columns)\n",
    "    log_dataset_op(op_path=\"input_path\",op_type=\"read\",data=data) # <- log info about the read\n",
    "\n",
    "\n",
    "    data = data.filter(col(\"transaction_amount\") > 100)\n",
    "    log_dataset_op(op_path=\"input_path\",op_type=\"write\",data=data) # <- log info about the read\n",
    "\n",
    "    # Data manipulations\n",
    "    data = data.filter(col(\"transaction_amount\") > 100)\n",
    "    log_dataset_op(op_path=\"output_path\",op_type=\"write\",data=data) # <- log info about the write\n",
    "\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databand_TestBook_1_Success",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
