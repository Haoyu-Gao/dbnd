ARG DOCKER_IMAGE_BASE=us.gcr.io/dbnd-dev-260010/databand/dbnd-airflow.base:py36-airflow-1-10-10-latest
FROM ${DOCKER_IMAGE_BASE}

USER root
# Never prompts the user for choices on installation/configuration of packages

ENV VIRTUALENV_NO_DOWNLOAD yes
ARG AIRFLOW_FOLDER=/usr/local/airflow
ARG PLUGINS_FOLDER=${AIRFLOW_FOLDER}/plugins

ENV SPARK_VERSION=2.4.5
ENV HADOOP_VERSION=2.7
ENV LIVY_VERSION=0.6.0
ARG AIRFLOW_VERSION=1.10.10


# INSTALL SPARK
# Add repository for openjdk-8-jdk-headless in debian
RUN echo 'deb http://ftp.us.debian.org/debian/ stretch main contrib non-free' > /etc/apt/sources.list.d/stretch.list
# Run the following commands on my Linux machine
RUN mkdir -p /usr/share/man/man1 && apt-get update -qq && apt-get upgrade -qq && \
    apt-get install -qq -y gnupg2 gcc wget unzip procps openjdk-8-jdk-headless scala vim

#Download the Spark binaries from the repo
WORKDIR /
#install spark binaries
RUN wget --no-verbose https://dbnd-dev-playground.s3.amazonaws.com/packages/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar -xvzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    echo "export PATH=$PATH:/spark/bin" >> ~/.bashrc && echo "export SPARK_HOME=/spark" >> ~/.bashrc && echo "export HADOOP_CONF_DIR=/etc/hadoop/conf" >> ~/.bashrc && \
    pip install pyspark==2.4.4
# EO SPARK INSTALL

# 'setuptools<58' is needed for Airflow Flask-OpenID dependency (2to3)
RUN pip install -U 'pip>=20,<20.3' 'setuptools<58'

# EO LOCAL VERSION
# DEV ENV

RUN pip install statsd matplotlib scikit-learn scipy==1.1.0
## AWS:
RUN pip install boto3 s3fs
## GCP:
RUN pip install google-api-python-client google-cloud google-cloud-storage
RUN pip install --upgrade "snowflake-connector-python<2.6.0"


# USING LOCAL VERSION OF DBND
COPY ./dbnd-core/dist-python/dbnd.requirements.txt \
    ./dbnd-core/dist-python/dbnd-airflow.requirements.txt \
    ./dbnd-core/dist-python/dbnd-airflow-monitor.requirements.txt \
    ./dbnd-core/dist-python/dbnd-mlflow.requirements.txt \
    ./dbnd-core/dist-python/dbnd-postgres.requirements.txt \
    ./dbnd-core/dist-python/dbnd-redshift.requirements.txt \
    ./dbnd-core/dist-python/dbnd-snowflake.requirements.txt \
    ./dbnd-core/dist-python/dbnd-luigi.requirements.txt \
    /dist-python/


RUN SHORT_PYTHON_VERSION=$(echo ${PYTHON_VERSION} | cut -f1,2 -d'.') && \
    pip install apache-airflow[postgres,mysql]==$AIRFLOW_VERSION \
        --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${SHORT_PYTHON_VERSION}.txt"

RUN pip install -r /dist-python/dbnd.requirements.txt  \
     -r /dist-python/dbnd-airflow.requirements.txt  \
     -r /dist-python/dbnd-airflow.requirements.txt  \
     -r /dist-python/dbnd-airflow-monitor.requirements.txt  \
     -r /dist-python/dbnd-mlflow.requirements.txt  \
     -r /dist-python/dbnd-postgres.requirements.txt  \
     -r /dist-python/dbnd-redshift.requirements.txt  \
     -r /dist-python/dbnd-snowflake.requirements.txt \
     -r /dist-python/dbnd-luigi.requirements.txt

COPY ./dbnd-core/dist-python/databand-*.whl \
    ./dbnd-core/dist-python/dbnd-*.whl \
    ./dbnd-core/dist-python/dbnd_mlflow-*.whl \
    ./dbnd-core/dist-python/dbnd_airflow_auto_tracking*.whl \
    ./dbnd-core/dist-python/dbnd_spark-*.whl \
    ./dbnd-core/dist-python/dbnd_postgres-*.whl \
    ./dbnd-core/dist-python/dbnd_redshift-*.whl \
    ./dbnd-core/dist-python/dbnd_luigi-*.whl \
    ./dbnd-core/dist-python/dbnd_snowflake-*.whl \
    ./dbnd-core/dist-python/dbnd_airflow-*.whl \
    ./dbnd-core/dist-python/dbnd_airflow_monitor-*.whl \
    /dist-python/
RUN pip install databand[mlflow,airflow-auto-tracking,spark,postgres,redshift,luigi,snowflake] dbnd-airflow --no-index --find-links /dist-python/
RUN touch project.cfg

COPY ./dbnd-core/setup.cfg /usr/local/airflow/databand/dbnd-core/setup.cfg

# Debug
RUN ls ${AIRFLOW_FOLDER}
USER airflow
WORKDIR ${AIRFLOW_FOLDER}

ARG SOURCE_VERSION

# DBND CONFIG
ENV DBND_HOME=${AIRFLOW_FOLDER}
ENV DBND__RUN_INFO__SOURCE_VERSION=${SOURCE_VERSION}
ENV DBND__MLFLOW_TRACKING__DATABAND_TRACKING=True

RUN echo "export PATH=$PATH:/spark/bin" >> ~/.bashrc && echo "export SPARK_HOME=/spark" >> ~/.bashrc
